{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e25d55f",
   "metadata": {},
   "source": [
    "# Customer Growth and Retention Analysis\n",
    "\n",
    "## Business Context\n",
    "This notebook implements a comprehensive customer retention strategy for a fintech/subscription/e-commerce company facing high Customer Acquisition Cost (CAC), elevated churn rates among high-value customers, and inefficient mass retention campaigns.\n",
    "\n",
    "**Objectives:**\n",
    "- Reduce churn\n",
    "- Maximize Customer Lifetime Value (CLV)\n",
    "- Avoid over-treatment (spam) of customers who don't need retention\n",
    "\n",
    "**Dataset:**\n",
    "- `customers.csv`: Customer IDs, signup dates, true lifetime days\n",
    "- `transactions.csv`: Transaction history with dates and amounts\n",
    "\n",
    "**Approach:**\n",
    "RFM analysis, churn prediction models, BG-NBD for transactional behavior, survival analysis for time-to-churn, and CLV modeling to prioritize retention efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db0194",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lifetimes import BetaGeoFitter, GammaGammaFitter\n",
    "from lifelines import CoxPHFitter, WeibullFitter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e92f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_customers(filepath='data/customers.csv'):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['signup_date'] = pd.to_datetime(df['signup_date'])\n",
    "    return df\n",
    "\n",
    "def load_transactions(filepath='data/transactions.csv'):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "    return df\n",
    "\n",
    "customers = load_customers()\n",
    "transactions = load_transactions()\n",
    "\n",
    "print(\"Customers shape:\", customers.shape)\n",
    "print(\"Transactions shape:\", transactions.shape)\n",
    "print(\"Observation date:\", transactions['transaction_date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479d9fa",
   "metadata": {},
   "source": [
    "- The dataset contains 3,000 customers with 46,706 transactions observed up to December 31, 2025. This provides approximately 1 year of transactional data (15.6 transactions per customer on average), which is sufficient for robust RFM and CLV analysis. The data appears clean with proper datetime parsing.\n",
    "\n",
    "- A dataset of this size and duration supports reliable customer segmentation and predictive modeling. The high transaction volume per customer suggests an engaged user base, but the observation date being in the future (2026) indicates this may be synthetic data for demonstration purposes. In real scenarios, ensure data covers at least 12-24 months for stable CLV estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb35b0e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "**Objectives:**\n",
    "- Understand data distributions and patterns\n",
    "- Identify outliers and data quality issues\n",
    "- Explore customer behavior patterns\n",
    "- Visualize key metrics and relationships\n",
    "\n",
    "**Analysis Focus:**\n",
    "- Transaction patterns over time\n",
    "- Customer demographics and behavior\n",
    "- Amount distributions and correlations\n",
    "- Temporal trends and seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdf99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total customers: {customers.shape[0]}\")\n",
    "print(f\"Total transactions: {transactions.shape[0]}\")\n",
    "print(f\"Average transactions per customer: {transactions.shape[0]/customers.shape[0]:.1f}\")\n",
    "print(f\"Date range: {transactions['transaction_date'].min()} to {transactions['transaction_date'].max()}\")\n",
    "print(f\"Observation period: {(transactions['transaction_date'].max() - transactions['transaction_date'].min()).days} days\")\n",
    "\n",
    "print(\"\\n=== CUSTOMER LIFETIME ANALYSIS ===\")\n",
    "observation_date = transactions['transaction_date'].max()\n",
    "customers['lifetime_days'] = (observation_date - customers['signup_date']).dt.days\n",
    "print(f\"Average customer lifetime: {customers['lifetime_days'].mean():.1f} days\")\n",
    "print(f\"Median customer lifetime: {customers['lifetime_days'].median():.1f} days\")\n",
    "print(f\"Min/Max lifetime: {customers['lifetime_days'].min()}/{customers['lifetime_days'].max()} days\")\n",
    "\n",
    "print(\"\\n=== TRANSACTION AMOUNT ANALYSIS ===\")\n",
    "print(f\"Average transaction amount: ${transactions['amount'].mean():.2f}\")\n",
    "print(f\"Median transaction amount: ${transactions['amount'].median():.2f}\")\n",
    "print(f\"Total revenue: ${transactions['amount'].sum():.2f}\")\n",
    "print(f\"Min/Max transaction: ${transactions['amount'].min():.2f}/${transactions['amount'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction amount distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(transactions['amount'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Transaction Amount Distribution')\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot(transactions['amount'])\n",
    "plt.title('Transaction Amount Box Plot')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "amounts_by_customer = transactions.groupby('customer_id')['amount'].sum()\n",
    "plt.hist(amounts_by_customer, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "plt.title('Total Spend per Customer')\n",
    "plt.xlabel('Total Amount ($)')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748c222",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Distribution Skewness: Transaction amounts and total spend per customer are heavily right-skewed (most values under $100 and $1,000 respectively), with long tails of high-value outliers. Frequency distributions show most customers have low transaction counts, indicating a power-law pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction frequency analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "freq_by_customer = transactions.groupby('customer_id').size()\n",
    "plt.hist(freq_by_customer, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "plt.title('Transactions per Customer')\n",
    "plt.xlabel('Number of Transactions')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(freq_by_customer, amounts_by_customer, alpha=0.6, color='purple')\n",
    "plt.title('Frequency vs Total Spend')\n",
    "plt.xlabel('Transaction Count')\n",
    "plt.ylabel('Total Amount ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation analysis\n",
    "plt.subplot(2, 2, 3)\n",
    "customer_metrics = transactions.groupby('customer_id').agg({\n",
    "    'amount': ['sum', 'mean', 'count'],\n",
    "    'transaction_date': ['min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "customer_metrics.columns = ['customer_id', 'total_spend', 'avg_amount', 'transaction_count', 'first_transaction', 'last_transaction']\n",
    "customer_metrics = pd.merge(customer_metrics, customers[['customer_id', 'signup_date']], on='customer_id')\n",
    "customer_metrics['days_since_signup'] = (observation_date - customer_metrics['signup_date']).dt.days\n",
    "customer_metrics['days_active'] = (customer_metrics['last_transaction'] - customer_metrics['first_transaction']).dt.days\n",
    "\n",
    "correlation_vars = ['total_spend', 'avg_amount', 'transaction_count', 'days_since_signup', 'days_active']\n",
    "corr_matrix = customer_metrics[correlation_vars].corr()\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', square=True)\n",
    "plt.title('Customer Metrics Correlation Matrix')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "avg_amount_by_freq = transactions.groupby('customer_id').agg({'amount': ['count', 'mean']})\n",
    "avg_amount_by_freq.columns = ['count', 'avg_amount']\n",
    "plt.scatter(avg_amount_by_freq['count'], avg_amount_by_freq['avg_amount'], alpha=0.6, color='red')\n",
    "plt.title('Transaction Frequency vs Average Amount')\n",
    "plt.xlabel('Transaction Count')\n",
    "plt.ylabel('Average Amount ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbe059",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Transaction Frequency Distribution: The histogram shows a highly skewed distribution with most customers having low transaction counts (under 20), while a few outliers have very high frequencies (up to 75). This indicates a power-law pattern typical in e-commerce, where a small number of highly engaged customers drive significant activity.\n",
    "- Frequency vs Total Spend: The scatter plot reveals a strong positive correlation (0.78), meaning customers with more transactions tend to spend more overall. However, there are high-spend customers with moderate frequency, suggesting occasional large purchases.\n",
    "- Frequency vs Average Amount: The third plot shows no clear correlation between transaction count and average spend per transaction, with average amounts clustering around $20-100 regardless of frequency. This suggests that frequent buyers don't necessarily spend more per transaction, highlighting opportunities for upselling to high-frequency, low-average-spend customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Daily transaction volume\n",
    "plt.subplot(2, 2, 1)\n",
    "daily_transactions = transactions.groupby('transaction_date').size()\n",
    "daily_transactions.plot(color='blue', linewidth=1)\n",
    "plt.title('Daily Transaction Volume')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Daily revenue\n",
    "plt.subplot(2, 2, 2)\n",
    "daily_revenue = transactions.groupby('transaction_date')['amount'].sum()\n",
    "daily_revenue.plot(color='green', linewidth=1)\n",
    "plt.title('Daily Revenue')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly patterns\n",
    "plt.subplot(2, 2, 3)\n",
    "monthly_transactions = transactions.groupby(pd.Grouper(key='transaction_date', freq='M')).size()\n",
    "monthly_transactions.plot(kind='bar', color='skyblue')\n",
    "plt.title('Monthly Transaction Volume')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Transactions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Customer signup patterns\n",
    "plt.subplot(2, 2, 4)\n",
    "monthly_signups = customers.groupby(pd.Grouper(key='signup_date', freq='M')).size()\n",
    "monthly_signups.plot(kind='bar', color='lightcoral')\n",
    "plt.title('Monthly Customer Signups')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('New Customers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475dcf2",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- Upward trend: Daily transactions increase almost linearly through the year, indicating improving acquisition/usage or expanding customer activity.\n",
    "- Late-year acceleration: Both transactions and revenue spike in Nov–Dec — investigate promotions, seasonality (holidays), or product/marketing changes.\n",
    "- Volatility in revenue: Daily revenue fluctuates more than transaction count → per-transaction amounts vary (occasional big orders).\n",
    "- Stable acquisition: Monthly signups are steady, so growth is likely driven by increased frequency / repeat usage from existing customers rather than big increases in new-user volume.\n",
    "- Partial / censored month: Very low December signups imply incomplete data for that month (right-censoring) — treat December differently when modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ee619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer lifetime distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(customers['lifetime_days'], bins=50, alpha=0.7, color='teal', edgecolor='black')\n",
    "plt.title('Customer Lifetime Distribution')\n",
    "plt.xlabel('Lifetime (days)')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e7b31",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Temporal Patterns: Daily transaction volume and revenue show steady trends without strong seasonality, but monthly signups and transactions reveal potential growth cycles. Customer lifetime is widely distributed, with many short-term users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33078e6d",
   "metadata": {},
   "source": [
    "## 2. Customer Value Foundations: RFM Analysis\n",
    "\n",
    "**RFM Framework:**\n",
    "- **Recency (R):** Days since last transaction\n",
    "- **Frequency (F):** Number of transactions\n",
    "- **Monetary (M):** Total amount spent\n",
    "\n",
    "**Decision: Scoring Method**\n",
    "- **Quantiles (1-5 scale) over equal bins:** Handles skewed distributions better.\n",
    "- **Reverse R scoring:** Lower recency = higher score (better).\n",
    "- **Comparison:** Equal bins might not capture distribution nuances.\n",
    "\n",
    "**Segmentation Logic:**\n",
    "- Champions: High R,F,M\n",
    "- Loyal: High F\n",
    "- At Risk: Low R, High F\n",
    "- Hibernating: Low R,F,M\n",
    "\n",
    "**Decision: Segmentation**\n",
    "- **Standard RFM segments over custom:** Actionable, industry-standard.\n",
    "- **Reasoning:** Provides clear retention strategies per segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d620669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM Calculation\n",
    "observation_date = transactions['transaction_date'].max()\n",
    "\n",
    "rfm = transactions.groupby('customer_id').agg(\n",
    "    recency=('transaction_date', lambda x: (observation_date - x.max()).days),\n",
    "    frequency=('transaction_date', 'count'),\n",
    "    monetary=('amount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Score RFM\n",
    "rfm['R_score'] = pd.qcut(rfm['recency'], 5, labels=[5,4,3,2,1])\n",
    "rfm['F_score'] = pd.qcut(rfm['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "rfm['M_score'] = pd.qcut(rfm['monetary'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "\n",
    "rfm['RFM_score'] = rfm['R_score'].astype(str) + rfm['F_score'].astype(str) + rfm['M_score'].astype(str)\n",
    "\n",
    "# Segmentation\n",
    "def segment_customer(row):\n",
    "    r, f, m = int(row['R_score']), int(row['F_score']), int(row['M_score'])\n",
    "    if r >= 4 and f >= 4 and m >= 4:\n",
    "        return 'Champions'\n",
    "    elif f >= 4:\n",
    "        return 'Loyal Customers'\n",
    "    elif r >= 4 and f >= 3:\n",
    "        return 'Potential Loyalists'\n",
    "    elif r >= 3 and f >= 3 and m >= 3:\n",
    "        return 'Promising'\n",
    "    elif r >= 3 and f <= 2:\n",
    "        return 'Needs Attention'\n",
    "    elif r <= 2 and f >= 3:\n",
    "        return 'At Risk'\n",
    "    elif r <= 2 and f <= 2:\n",
    "        return 'Hibernating'\n",
    "    else:\n",
    "        return 'Others'\n",
    "\n",
    "rfm['segment'] = rfm.apply(segment_customer, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.bar(rfm['segment'].value_counts().index, rfm['segment'].value_counts().values, color='teal', edgecolor='black')\n",
    "plt.title('RFM Customer Segments')\n",
    "plt.xlabel('Segment')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d30f80",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- The RFM segmentation reveals a heavily skewed customer base with 716 customers (23.9%) classified as \"Hibernating\" (low recency, frequency, monetary), indicating significant churn risk. Only 619 customers (20.6%) are \"Champions,\" representing the high-value core. The distribution suggests opportunities for reactivation campaigns targeting low-engagement segments while nurturing high-value customers.\n",
    "- The segmentation provides actionable customer groups for marketing allocation. With 20.6% Champions driving likely disproportionate revenue, protecting this segment should be a priority. The large Hibernating group suggests either data quality issues (if synthetic) or significant business challenges in customer retention, potentially impacting long-term revenue stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e22cfc",
   "metadata": {},
   "source": [
    "## 3. Churn Definition\n",
    "\n",
    "**Churn Definition:**\n",
    "- Churn = 1 if no transaction in last 60 days from observation date\n",
    "- Churn = 0 otherwise\n",
    "\n",
    "**Decision: 60-day Window**\n",
    "- **Why 60 days?** Balances early detection (vs. 90 days) and reduces false positives (vs. 30 days).\n",
    "- **Comparison:** 30 days too sensitive (temporary inactivity), 90 days too late for intervention.\n",
    "- **Reasoning:** Common in subscription businesses for monthly cycles.\n",
    "\n",
    "**For Classification:** Use this as target variable.  \n",
    "**For Survival:** Duration from signup to churn/censoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aefb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define churn for classification\n",
    "churn_df = transactions.groupby('customer_id').agg(\n",
    "    last_transaction=('transaction_date', 'max'),\n",
    "    frequency=('transaction_date', 'count'),\n",
    "    monetary=('amount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "churn_df['recency'] = (observation_date - churn_df['last_transaction']).dt.days\n",
    "churn_df['churn'] = (churn_df['recency'] > 60).astype(int)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "counts = churn_df['churn'].value_counts().sort_index()  # ensure order 0,1\n",
    "bars = plt.bar(counts.index, counts.values, color='salmon', edgecolor='black')\n",
    "plt.title('Churn Distribution')\n",
    "plt.xlabel('Churn (1=Churned, 0=Active)')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks([0, 1], ['Active', 'Churned'])\n",
    "\n",
    "# annotate each bar with its count\n",
    "ax = plt.gca()\n",
    "for bar in bars:\n",
    "    h = bar.get_height()\n",
    "    ax.annotate(f'{int(h)}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, h),\n",
    "                xytext=(0, 5), textcoords='offset points',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe2738",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- A perfectly balanced 50/50 churn split (1,501 churners vs. 1,499 non-churners) is highly unusual for real customer data, where churn rates typically range from 5-20%. The 60-day inactivity threshold captures customers who haven't transacted recently, providing a practical operational definition.\n",
    "- The balanced classes enable robust model training but may not reflect real-world churn dynamics. In production, imbalanced classes would require techniques like class weighting or resampling. The 50% churn rate indicates a critical business problem requiring immediate attention, potentially signaling product-market fit issues or competitive pressures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe304e0",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "**Additional Features:**\n",
    "- Trend features\n",
    "    - 7 / 30 / 90-day rolling mean and rolling std for transactions and revenue (captures momentum and volatility).\n",
    "    - Slope / linear trend over last 30 and 90 days (rate of change).\n",
    "- Recent-activity windows\n",
    "    - Transactions_last_30, Transactions_last_90, Revenue_last_30, Revenue_last_90.\n",
    "    - Unique_active_days_last_30 (engagement density).\n",
    "- Cohort & lifecycle\n",
    "    - Customer_cohort_month (signup month) and cohort_age_days; cohort retention rate per month.\n",
    "    - Lifetime_days and days_since_last_txn (recency).\n",
    "- AOV and spend dispersion\n",
    "    - Avg_amount_last_30, Median_amount, Max_amount, Num_large_txns (>95th percentile).\n",
    "    - Flag_high_value_outlier (binary).\n",
    "- Frequency / cadence\n",
    "    - Avg_interpurchase_days, Std_interpurchase_days, last_interpurchase_days.\n",
    "    - Frequency_ratio = transactions_last_30 / transactions_last_90 (acceleration).\n",
    "- Seasonality & calendar\n",
    "    - Is_holiday_window (Nov–Dec, Black Friday week), day_of_week, month_of_year dummies.\n",
    "- Risk & engagement signals\n",
    "    - Percent_change_transactions_month_over_month, churn_risk_proxy (recency > X days)\n",
    "- Robust transforms\n",
    "    - Log(1 + revenue/amount/total_spend) for monetary features to reduce skew.\n",
    "    - Winsorize or clip monetary features at 99th percentile for tree-based models; keep outlier flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298693a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(transactions, customers, cutoff_date, label_window_days=60):\n",
    "    \"\"\"\n",
    "    Build features using only transactions <= cutoff_date.\n",
    "    Label churn = 1 if NO transaction in (cutoff_date, cutoff_date + label_window_days].\n",
    "    Returns a DataFrame indexed by customer_id with engineered features and 'churn' column.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from lifetimes import BetaGeoFitter\n",
    "\n",
    "    # limit transactions to data available at cutoff\n",
    "    tx_cut = transactions[transactions['transaction_date'] <= cutoff_date].copy()\n",
    "    obs = pd.to_datetime(cutoff_date)\n",
    "\n",
    "    # base churn_df from available data (up to cutoff)\n",
    "    churn_df_cut = tx_cut.groupby('customer_id').agg(\n",
    "        last_transaction=('transaction_date', 'max'),\n",
    "        frequency=('transaction_date', 'count'),\n",
    "        monetary=('amount', 'sum')\n",
    "    ).reset_index()\n",
    "    churn_df_cut['recency'] = (obs - churn_df_cut['last_transaction']).dt.days\n",
    "\n",
    "    # ensure we include all customers (even those with no tx up to cutoff)\n",
    "    cust_ids = customers['customer_id'].unique()\n",
    "\n",
    "    # helper window boundaries relative to cutoff\n",
    "    last_30_start = obs - pd.Timedelta(days=30)\n",
    "    last_90_start = obs - pd.Timedelta(days=90)\n",
    "    prev_30_start = obs - pd.Timedelta(days=60)\n",
    "    prev_30_end = obs - pd.Timedelta(days=30)\n",
    "\n",
    "    tx = tx_cut  # shorthand\n",
    "\n",
    "    # Windowed aggregates\n",
    "    tx_30 = tx[tx['transaction_date'] >= last_30_start].groupby('customer_id').agg(\n",
    "        transactions_last_30=('transaction_date', 'count'),\n",
    "        revenue_last_30=('amount', 'sum'),\n",
    "        avg_amount_last_30=('amount', 'mean'),\n",
    "        unique_active_days_last_30=('transaction_date', lambda x: x.dt.date.nunique())\n",
    "    )\n",
    "\n",
    "    tx_90 = tx[tx['transaction_date'] >= last_90_start].groupby('customer_id').agg(\n",
    "        transactions_last_90=('transaction_date', 'count'),\n",
    "        revenue_last_90=('amount', 'sum')\n",
    "    )\n",
    "\n",
    "    tx_prev30 = tx[(tx['transaction_date'] >= prev_30_start) & (tx['transaction_date'] < prev_30_end)].groupby('customer_id').agg(\n",
    "        transactions_prev_30=('transaction_date', 'count'),\n",
    "        revenue_prev_30=('amount', 'sum')\n",
    "    )\n",
    "\n",
    "    # Inter-purchase stats (vectorized)\n",
    "    tx_sorted = tx.sort_values(['customer_id', 'transaction_date'])\n",
    "    diffs = tx_sorted.groupby('customer_id')['transaction_date'].diff().dt.days\n",
    "    avg_interpurchase = diffs.groupby(tx_sorted['customer_id']).mean().rename('avg_interpurchase_days')\n",
    "    std_interpurchase = diffs.groupby(tx_sorted['customer_id']).std().rename('std_interpurchase_days')\n",
    "    last_interpurchase = diffs.groupby(tx_sorted['customer_id']).last().rename('last_interpurchase_days')\n",
    "\n",
    "    # Per-customer amount dispersion\n",
    "    cust_median = tx.groupby('customer_id')['amount'].median().rename('median_amount')\n",
    "    cust_max = tx.groupby('customer_id')['amount'].max().rename('max_amount')\n",
    "    cust_avg_overall = tx.groupby('customer_id')['amount'].mean().rename('avg_amount_overall')\n",
    "\n",
    "    # Large-transaction flags (computed on available data)\n",
    "    large_threshold = tx['amount'].quantile(0.95) if not tx.empty else 0.0\n",
    "    num_large = tx[tx['amount'] > large_threshold].groupby('customer_id').size().rename('num_large_txns')\n",
    "    flag_high_value = (num_large > 0).rename('flag_high_value_outlier')\n",
    "\n",
    "    # Cohort & lifecycle (relative to cutoff)\n",
    "    cohort_month = customers.set_index('customer_id')['signup_date'].dt.to_period('M').rename('customer_cohort_month')\n",
    "    lifetime_days = (pd.to_datetime(obs) - customers.set_index('customer_id')['signup_date']).dt.days.rename('lifetime_days')\n",
    "\n",
    "    # Last transaction features (from available data)\n",
    "    last_tx = tx.groupby('customer_id')['transaction_date'].max().rename('last_transaction')\n",
    "    last_tx_month = last_tx.dt.month.rename('last_tx_month')\n",
    "    last_tx_dow = last_tx.dt.dayofweek.rename('last_tx_day_of_week')\n",
    "    is_holiday_window = last_tx_month.isin([11, 12]).rename('is_holiday_window')\n",
    "\n",
    "    # Aggregate / ratio features (use churn_df_cut where available)\n",
    "    total_spend = churn_df_cut.set_index('customer_id')['monetary'].rename('total_spend')\n",
    "    frequency = churn_df_cut.set_index('customer_id')['frequency'].rename('frequency')\n",
    "    avg_amount_calc = (total_spend / frequency).replace([np.inf, -np.inf], np.nan).rename('avg_amount_calc')\n",
    "\n",
    "    # Percent change (last 30 vs previous 30)\n",
    "    pct_change_tx = ((tx_30.get('transactions_last_30', pd.Series(dtype=float)).fillna(0) -\n",
    "                      tx_prev30.get('transactions_prev_30', pd.Series(dtype=float)).fillna(0)) /\n",
    "                     (tx_prev30.get('transactions_prev_30', pd.Series(dtype=float)).replace(0, np.nan).fillna(1))\n",
    "                    ).rename('pct_change_tx_last30_vs_prev30')\n",
    "\n",
    "    # Frequency acceleration\n",
    "    freq_ratio = (tx_30.get('transactions_last_30', pd.Series(dtype=float)).fillna(0) /\n",
    "                  tx_90.get('transactions_last_90', pd.Series(dtype=float)).replace(0, np.nan).fillna(1)\n",
    "                 ).rename('frequency_ratio_30_90')\n",
    "\n",
    "    # Robust transforms & winsorize (on available data)\n",
    "    cap_99 = tx['amount'].quantile(0.99) if not tx.empty else 0.0\n",
    "\n",
    "    # Prepare DataFrame of computed features and join series (indexed by customer_id)\n",
    "    computed = pd.DataFrame(index=cust_ids)\n",
    "    join_list = [\n",
    "        tx_30[['transactions_last_30', 'revenue_last_30', 'avg_amount_last_30', 'unique_active_days_last_30']].reindex(computed.index),\n",
    "        tx_90[['transactions_last_90', 'revenue_last_90']].reindex(computed.index),\n",
    "        tx_prev30[['transactions_prev_30', 'revenue_prev_30']].reindex(computed.index),\n",
    "        avg_interpurchase.reindex(computed.index),\n",
    "        std_interpurchase.reindex(computed.index),\n",
    "        last_interpurchase.reindex(computed.index),\n",
    "        cust_median.reindex(computed.index),\n",
    "        cust_max.reindex(computed.index),\n",
    "        cust_avg_overall.reindex(computed.index),\n",
    "        num_large.reindex(computed.index),\n",
    "        flag_high_value.reindex(computed.index),\n",
    "        last_tx.reindex(computed.index),\n",
    "        last_tx_month.reindex(computed.index),\n",
    "        last_tx_dow.reindex(computed.index),\n",
    "        is_holiday_window.reindex(computed.index),\n",
    "        cohort_month.reindex(computed.index),\n",
    "        lifetime_days.reindex(computed.index),\n",
    "        total_spend.reindex(computed.index),\n",
    "        frequency.reindex(computed.index),\n",
    "        avg_amount_calc.reindex(computed.index)\n",
    "    ]\n",
    "    computed = computed.join(join_list).fillna(0)\n",
    "\n",
    "    # Ensure expected cols exist\n",
    "    for col in ['transactions_last_30', 'revenue_last_30', 'avg_amount_last_30', 'unique_active_days_last_30',\n",
    "                'transactions_last_90', 'revenue_last_90', 'transactions_prev_30', 'revenue_prev_30']:\n",
    "        if col not in computed.columns:\n",
    "            computed[col] = 0\n",
    "\n",
    "    # Add ratio / percent-change features safely\n",
    "    computed['pct_change_tx_last30_vs_prev30'] = pct_change_tx.reindex(computed.index).fillna(0)\n",
    "    computed['frequency_ratio_30_90'] = freq_ratio.reindex(computed.index).fillna(0)\n",
    "\n",
    "    # Winsorize monetary-related features at 99th percentile (cap_99 computed on tx_cut)\n",
    "    for c in ['avg_amount_overall', 'avg_amount_last_30', 'median_amount', 'max_amount', 'revenue_last_30', 'revenue_last_90', 'total_spend']:\n",
    "        if c in computed.columns:\n",
    "            computed[c] = computed[c].clip(upper=cap_99)\n",
    "\n",
    "    # Log transforms\n",
    "    computed['log_total_spend'] = np.log1p(computed.get('total_spend', 0))\n",
    "    computed['log_revenue_last_30'] = np.log1p(computed.get('revenue_last_30', 0))\n",
    "    computed['log_avg_amount_last_30'] = np.log1p(computed.get('avg_amount_last_30', 0))\n",
    "\n",
    "    # Merge churn_df_cut values (recency, frequency, monetary, last_transaction) into computed features\n",
    "    churn_indexed = churn_df_cut.set_index('customer_id')\n",
    "    # columns to bring from churn_df_cut\n",
    "    for c in ['recency', 'frequency', 'monetary']:\n",
    "        computed[c] = churn_indexed.get(c).reindex(computed.index).fillna(0)\n",
    "\n",
    "    # Add signup day_of_week/month for seasonality (relative to signup_date)\n",
    "    signup_df = customers.set_index('customer_id')[['signup_date']].copy()\n",
    "    signup_df['signup_day_of_week'] = signup_df['signup_date'].dt.dayofweek\n",
    "    signup_df['signup_month'] = signup_df['signup_date'].dt.month\n",
    "    computed = computed.join(signup_df[['signup_day_of_week', 'signup_month']].reindex(computed.index)).fillna(0)\n",
    "\n",
    "    # Compute churn label using transactions AFTER cutoff within the label window\n",
    "    future_start = obs\n",
    "    future_end = obs + pd.Timedelta(days=label_window_days)\n",
    "    future_tx = transactions[(transactions['transaction_date'] > future_start) &\n",
    "                             (transactions['transaction_date'] <= future_end)]\n",
    "    had_future_tx = future_tx.groupby('customer_id').size().rename('had_future_tx')\n",
    "    # churn = 1 if NO future tx in label window\n",
    "    computed['churn'] = (~computed.index.isin(had_future_tx.index)).astype(int)\n",
    "\n",
    "    # Reset index to have customer_id as column for consistency with notebook\n",
    "    features_df = computed.reset_index().rename(columns={'index': 'customer_id'})\n",
    "\n",
    "    # Final cleaning\n",
    "    features_df.fillna(0, inplace=True)\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc7dc6",
   "metadata": {},
   "source": [
    "## 5. Churn Prediction: Classification Models\n",
    "\n",
    "**Models:**\n",
    "- Logistic Regression: Interpretable, probabilistic\n",
    "- Random Forest: Handles non-linearity, feature importance\n",
    "\n",
    "**Decision: Model Selection**\n",
    "- **Logistic + RF over SVM/NN:** Logistic for baseline, RF for complexity; SVM slow, NN overkill.\n",
    "- **Class weighting:** Handles imbalanced churn.\n",
    "- **Comparison:** RF often outperforms Logistic on tabular data.\n",
    "\n",
    "**Evaluation:**\n",
    "- AUC: Overall discriminatory power\n",
    "- Confusion Matrix: Class-specific performance\n",
    "- Feature Importance: Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_train = transactions['transaction_date'].max() - pd.Timedelta(days=180)\n",
    "cutoff_test  = transactions['transaction_date'].max() - pd.Timedelta(days=60)\n",
    "\n",
    "# Build features using only data up to each cutoff\n",
    "train_df = build_features(transactions, customers, cutoff_train, label_window_days=60)\n",
    "test_df  = build_features(transactions, customers, cutoff_test,  label_window_days=60)\n",
    "\n",
    "\n",
    "# Feature list used by notebook (will intersect with available columns)\n",
    "feature_cols = [\n",
    "    'transactions_last_30', 'revenue_last_30',\n",
    "    'avg_amount_last_30', 'unique_active_days_last_30',\n",
    "    'transactions_last_90', 'revenue_last_90', 'transactions_prev_30',\n",
    "    'revenue_prev_30', 'avg_interpurchase_days', 'std_interpurchase_days',\n",
    "    'last_interpurchase_days', 'median_amount', 'max_amount',\n",
    "    'avg_amount_overall', 'num_large_txns', 'flag_high_value_outlier',\n",
    "    'last_tx_month', 'last_tx_day_of_week',\n",
    "    'is_holiday_window', 'lifetime_days',\n",
    "    'total_spend', 'frequency', 'avg_amount_calc',\n",
    "    'pct_change_tx_last30_vs_prev30', 'frequency_ratio_30_90',\n",
    "    'log_total_spend', 'log_revenue_last_30', 'log_avg_amount_last_30',\n",
    "    'recency', 'monetary', 'signup_day_of_week', 'signup_month'\n",
    "]\n",
    "\n",
    "common_features = [c for c in feature_cols if c in train_df.columns and c in test_df.columns]\n",
    "\n",
    "X_train = train_df.set_index('customer_id')[common_features].fillna(0)\n",
    "y_train = train_df.set_index('customer_id')['churn']\n",
    "X_test  = test_df.set_index('customer_id')[common_features].fillna(0)\n",
    "y_test  = test_df.set_index('customer_id')['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# Models: Logistic, Random Forest, XGBoost\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "models = {}\n",
    "\n",
    "# Logistic Regression\n",
    "log_model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "log_model.fit(X_train_scaled, y_train)\n",
    "models['Logistic'] = {\n",
    "    'model': log_model,\n",
    "    'y_pred': log_model.predict(X_test_scaled),\n",
    "    'y_proba': log_model.predict_proba(X_test_scaled)[:, 1],\n",
    "    'coef': getattr(log_model, 'coef_', None)\n",
    "}\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "models['RandomForest'] = {\n",
    "    'model': rf_model,\n",
    "    'y_pred': rf_model.predict(X_test_scaled),\n",
    "    'y_proba': rf_model.predict_proba(X_test_scaled)[:, 1],\n",
    "    'feat_imp': getattr(rf_model, 'feature_importances_', None)\n",
    "}\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=True, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "models['XGBoost'] = {\n",
    "    'model': xgb_model,\n",
    "    'y_pred': xgb_model.predict(X_test_scaled),\n",
    "    'y_proba': xgb_model.predict_proba(X_test_scaled)[:, 1],\n",
    "    'feat_imp': getattr(xgb_model, 'feature_importances_', None)\n",
    "}\n",
    "\n",
    "# Evaluate and report metrics\n",
    "metrics_summary = []\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, info in models.items():\n",
    "    y_pred = info['y_pred']\n",
    "    y_proba = info['y_proba']\n",
    "    auc_score = roc_auc_score(y_test, y_proba)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    metrics_summary.append({'model': name, 'roc_auc': auc_score, 'precision': prec, 'recall': rec, 'confusion_matrix': cm})\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc_score:.3f})\")\n",
    "\n",
    "# ROC plot\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix heatmaps\n",
    "import math\n",
    "n_models = len(metrics_summary)\n",
    "cols = 2\n",
    "rows = math.ceil(n_models / cols)\n",
    "plt.figure(figsize=(6 * cols, 4 * rows))\n",
    "for i, m in enumerate(metrics_summary, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.heatmap(m['confusion_matrix'], annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f\"{m['model']} Confusion Matrix\\nPrecision={m['precision']:.3f} Recall={m['recall']:.3f}\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ce48e",
   "metadata": {},
   "source": [
    "- RandomForest is the best overall classifier (highest AUC) and gives a strong tradeoff between finding churners (recall) and not wasting interventions (precision).\n",
    "- XGBoost aggressively labels customers as churners → you’ll capture the most true churners but at the cost of many wasted retention actions (high FP). Useful if retention action is very cheap relative to losing a customer.\n",
    "- Logistic is more conservative (higher precision, lower recall) → fewer wasted actions but more missed churners. Good when retention cost is high or budget is tight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Feature importance / coefficient visualization\n",
    "# Build a combined DataFrame for importances\n",
    "feat_names = list(X_train.columns)\n",
    "imp_dfs = []\n",
    "for name, info in models.items():\n",
    "    if name == 'Logistic' and info.get('coef') is not None:\n",
    "        coefs = info['coef'].ravel()\n",
    "        df = pd.DataFrame({'feature': feat_names, 'importance': np.abs(coefs)})\n",
    "        df = df.sort_values('importance', ascending=False).head(20)\n",
    "        df['model'] = name\n",
    "        imp_dfs.append(df)\n",
    "    elif info.get('feat_imp') is not None:\n",
    "        imps = info['feat_imp']\n",
    "        df = pd.DataFrame({'feature': feat_names, 'importance': imps})\n",
    "        df = df.sort_values('importance', ascending=False).head(20)\n",
    "        df['model'] = name\n",
    "        imp_dfs.append(df)\n",
    "\n",
    "if imp_dfs:\n",
    "    imp_all = pd.concat(imp_dfs, ignore_index=True)\n",
    "    # Plot top features per model\n",
    "    models_present = imp_all['model'].unique()\n",
    "    plt.figure(figsize=(10, 4 * len(models_present)))\n",
    "    for i, m in enumerate(models_present, 1):\n",
    "        plt.subplot(len(models_present), 1, i)\n",
    "        subset = imp_all[imp_all['model'] == m].sort_values('importance', ascending=True)\n",
    "        plt.barh(subset['feature'], subset['importance'], color='tab:blue')\n",
    "        plt.title(f\"Top features - {m}\")\n",
    "        plt.xlabel('Importance (abs coef or feature_importance)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# feature names & scaled DataFrames used by models\n",
    "feat_names = list(X_train.columns)\n",
    "X_train_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=feat_names)\n",
    "X_test_df  = pd.DataFrame(X_test_scaled,  index=X_test.index,  columns=feat_names)\n",
    "\n",
    "# helper to extract one-class shap array for binary classifiers\n",
    "def _get_positive_shap(shap_vals):\n",
    "    if isinstance(shap_vals, list) or isinstance(shap_vals, tuple):\n",
    "        # shap returns list [class0, class1] for binary classifiers -> take positive class (index 1)\n",
    "        if len(shap_vals) == 2:\n",
    "            return shap_vals[1]\n",
    "        # otherwise try last\n",
    "        return shap_vals[-1]\n",
    "    return shap_vals\n",
    "\n",
    "# compute & plot SHAP for each model\n",
    "for name, info in models.items():\n",
    "    model = info['model']\n",
    "    print(f\"=== SHAP for model: {name} ===\")\n",
    "    try:\n",
    "        # Choose explainer\n",
    "        if hasattr(shap, \"TreeExplainer\") and (name.lower().find(\"random\") >= 0 or name.lower().find(\"xgboost\") >= 0 or type(model).__name__.lower().find(\"tree\")>=0):\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_vals = explainer.shap_values(X_test_df)\n",
    "            shap_arr = _get_positive_shap(shap_vals)\n",
    "        else:\n",
    "            # linear model -> use LinearExplainer (faster / more exact than Kernel for linear)\n",
    "            if hasattr(shap, \"LinearExplainer\"):\n",
    "                explainer = shap.LinearExplainer(model, X_train_df, feature_perturbation=\"interventional\")\n",
    "                shap_arr = explainer.shap_values(X_test_df)\n",
    "            else:\n",
    "                # fallback (slow) to KernelExplainer with small background sample\n",
    "                background = shap.sample(X_train_df, min(100, len(X_train_df)), random_state=42)\n",
    "                explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "                shap_vals = explainer.shap_values(X_test_df, nsamples=200)\n",
    "                shap_arr = _get_positive_shap(shap_vals)\n",
    "\n",
    "        try:\n",
    "            # shap.summary_plot works with numpy arrays + feature matrix/DF\n",
    "            shap.summary_plot(shap_arr, X_test_df, feature_names=feat_names, show=False)\n",
    "            plt.title(f\"SHAP summary - {name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Could not compute SHAP for {name}: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9124473",
   "metadata": {},
   "source": [
    "- All three models agree that recency / recent activity and lifetime/frequency/monetary signals are the most important predictors of churn. Differences are in which covariates and interactions each model exploits (seasonal cohort effects, non‑linearities, feature interactions).\n",
    "- XGBoost — key takeaways\n",
    "    - Dominant features: recency, signup_month, lifetime_days, last_tx_month, total_spend.\n",
    "    - Behavior: recency is a strong risk signal (higher recency → higher predicted churn). signup_month / last_tx_month create strong cohort/seasonality effects the tree model captures nonlinearly.\n",
    "    - Strength: captures non‑linear thresholds and cohort interactions (explains why signup_month is high importance).\n",
    "    - Action: inspect SHAP dependence plots for recency and signup_month; consider encoding cohort effects explicitly.\n",
    "- Random Forest — key takeaways\n",
    "    - Top signals include lifetime_days, signup_month and derived ratios (frequency_ratio_30_90), recent revenue / transactions.\n",
    "    - Evidence of interactions: SHAP interaction plot shows transactions_last_30 and revenue_last_30 interact — recent frequency + monetary together reduce churn risk more than either alone.\n",
    "    - Strength: robust to noisy features and captures interactions; importance magnitudes smaller than XGBoost (spread across features).\n",
    "    - Action: use interaction SHAP plots to design combined features (e.g., recent_freq × recent_rev) and validate stability across folds.\n",
    "- Logistic (linear) — key takeaways\n",
    "    - Top features by absolute coefficient: lifetime_days, recency, frequency, total_spend.\n",
    "    - Behavior: provides clear, interpretable directional effects (but assumes linear relationships).\n",
    "    - Limitation: may underperform when relationships are non‑linear or involve strong interactions (observed in tree-based models).\n",
    "    - Action: keep logistic for interpretability; add polynomial / binned features for recency/lifetime if linearity assumptions fail.\n",
    "- Cross‑model observations\n",
    "    - Recency / recent activity and lifetime are consistently most important — priority monitoring metrics.\n",
    "signup_month / last_tx_month = cohort/seasonality signals; check for data leakage or cohort effects (different acquisition cohorts behave differently).\n",
    "    - Some monetary features (total_spend, revenue_last_30) matter, but their effect is conditional on recent activity (interaction).\n",
    "    - Tree models (XGBoost/RF) reveal non‑linear and interaction effects that logistic does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6800ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_pct = 0.20\n",
    "n_test = len(y_test)\n",
    "top_k = max(1, int(top_pct * n_test))\n",
    "total_positives = int(y_test.sum())\n",
    "\n",
    "results = []\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for name, info in models.items():\n",
    "    probs = np.asarray(info['y_proba'])\n",
    "    y_true = np.asarray(y_test)  # aligned with X_test used to produce y_proba\n",
    "    df_scores = pd.DataFrame({'y_true': y_true, 'y_proba': probs})\n",
    "    df_sorted = df_scores.sort_values('y_proba', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # precision@top_k\n",
    "    top_slice = df_sorted.iloc[:top_k]\n",
    "    tp = int(top_slice['y_true'].sum())\n",
    "    fp = int(top_k - tp)\n",
    "    prec_at_k = tp / top_k\n",
    "    rec_at_k = tp / total_positives if total_positives > 0 else 0.0\n",
    "\n",
    "    # overall precision/recall from current predicted labels (if available)\n",
    "    y_pred = info.get('y_pred', None)\n",
    "    if y_pred is not None:\n",
    "        overall_prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        overall_rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    else:\n",
    "        overall_prec, overall_rec = (None, None)\n",
    "\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'top_k': top_k,\n",
    "        'TP_top_k': tp,\n",
    "        'FP_top_k': fp,\n",
    "        'precision_at_top_k': prec_at_k,\n",
    "        'recall_at_top_k': rec_at_k,\n",
    "        'overall_precision': overall_prec,\n",
    "        'overall_recall': overall_rec\n",
    "    })\n",
    "\n",
    "    # precision@k curve (compute precision for many k)\n",
    "    ks = np.linspace(0.01, 0.50, 50)  # examine top 1%..50%\n",
    "    precs = []\n",
    "    for p in ks:\n",
    "        k = max(1, int(p * n_test))\n",
    "        tp_k = int(df_sorted.iloc[:k]['y_true'].sum())\n",
    "        precs.append(tp_k / k)\n",
    "    plt.plot(ks * 100, precs, label=f\"{name}\")\n",
    "\n",
    "# show precision@k for models\n",
    "plt.xlabel('Top k (%) of customers targeted')\n",
    "plt.ylabel('Precision (fraction of true churners in targeted group)')\n",
    "plt.title('Precision@k curve (1%--50%)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2185609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table of results\n",
    "res_df = pd.DataFrame(results).set_index('model')\n",
    "print(f\"Precision@{int(top_pct*100)}% (top {top_k} customers):\")\n",
    "res_df[['TP_top_k','FP_top_k','precision_at_top_k','recall_at_top_k','overall_precision','overall_recall']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666134a0",
   "metadata": {},
   "source": [
    "- Precision@20% (top 600 customers): Logistic 0.995, RandomForest 0.985, XGBoost 0.988 — extremely high precision at operational top-k.\n",
    "- Recall@20% (capture at top 20%): ~0.39 for all models → top 20% recovers ≈39% of all churners.\n",
    "- Overall precision / recall tradeoffs:\n",
    "    - Logistic: precision 0.81, recall 0.76 (more conservative)\n",
    "    - RandomForest: precision 0.76, recall 0.83 (better recall)\n",
    "    - XGBoost: precision 0.61, recall 0.87 (more aggressive)\n",
    "- Extremely high precision at top-k implies outreach waste is minimal when using model-ranked lists (good for expensive interventions).\n",
    "- However, top-20% only captures ~39% of churners — if the objective is coverage (catch as many churners as possible), we need to target a larger fraction or change model threshold/strategy.\n",
    "- Logistic yields highest precision@20% (slightly) but lower overall recall than RF/XGBoost — choose logistic when intervention cost is high, RF/XGBoost when missing churners is more costly.\n",
    "- XGBoost’s higher overall recall but lower overall precision indicates it will find more churners but at the cost of more false positives outside the top-k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab90e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Settings\n",
    "top_pct = 0.20\n",
    "n_test = len(y_test)\n",
    "top_k = max(1, int(top_pct * n_test))\n",
    "total_positives = int(y_test.sum())\n",
    "baseline_rate = total_positives / n_test if n_test > 0 else 0.0\n",
    "\n",
    "# Compute gain / lift and plot Gain Chart + Lift Curve\n",
    "lift_rows = []\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Gain chart (cumulative capture)\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, info in models.items():\n",
    "    probs = np.asarray(info['y_proba'])\n",
    "    y_true = np.asarray(y_test)\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_proba': probs}).sort_values('y_proba', ascending=False).reset_index(drop=True)\n",
    "    df['cum_TP'] = df['y_true'].cumsum()\n",
    "    df['cum_pct_pop'] = (np.arange(1, len(df) + 1) / len(df))\n",
    "    df['cum_capture'] = df['cum_TP'] / total_positives if total_positives > 0 else 0.0\n",
    "\n",
    "    plt.plot(df['cum_pct_pop'] * 100, df['cum_capture'] * 100, label=name)\n",
    "\n",
    "    # metrics at top_k (20%)\n",
    "    tp_top = int(df.iloc[:top_k]['y_true'].sum())\n",
    "    prec_at_k = tp_top / top_k if top_k > 0 else 0.0\n",
    "    rec_at_k = tp_top / total_positives if total_positives > 0 else 0.0\n",
    "    lift_at_k = prec_at_k / baseline_rate if baseline_rate > 0 else np.nan\n",
    "    lift_rows.append({'model': name, 'TP_top_k': tp_top, 'precision_at_top_k': prec_at_k,\n",
    "                      'recall_at_top_k': rec_at_k, 'lift_at_top_k': lift_at_k})\n",
    "\n",
    "plt.plot([0, 100], [0, 100], 'k--', alpha=0.6, label='Random')\n",
    "plt.xlabel('Population targeted (%)')\n",
    "plt.ylabel('Cumulative captured positives (%)')\n",
    "plt.title('Gain Chart')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Lift curve\n",
    "plt.subplot(1, 2, 2)\n",
    "ks = np.linspace(0.01, 0.50, 50)\n",
    "for name, info in models.items():\n",
    "    probs = np.asarray(info['y_proba'])\n",
    "    y_true = np.asarray(y_test)\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_proba': probs}).sort_values('y_proba', ascending=False).reset_index(drop=True)\n",
    "    lifts = []\n",
    "    for p in ks:\n",
    "        k = max(1, int(p * len(df)))\n",
    "        prec_k = df.iloc[:k]['y_true'].mean() if k > 0 else 0.0\n",
    "        lifts.append(prec_k / baseline_rate if baseline_rate > 0 else np.nan)\n",
    "    plt.plot(ks * 100, lifts, label=name)\n",
    "\n",
    "plt.xlabel('Top k (%) of customers targeted')\n",
    "plt.ylabel('Lift (precision / baseline)')\n",
    "plt.title('Lift Curve')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show numeric lift@20% table\n",
    "lift_df = pd.DataFrame(lift_rows).set_index('model')\n",
    "print(f\"Baseline positive rate (overall): {baseline_rate:.3f} ({total_positives}/{n_test})\")\n",
    "print(f\"\\nLift / Gain metrics at top {int(top_pct*100)}% (top_k={top_k}):\")\n",
    "lift_df[['TP_top_k', 'precision_at_top_k', 'recall_at_top_k', 'lift_at_top_k']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c6f57",
   "metadata": {},
   "source": [
    "Lift Curve\n",
    "- The Plateau: The curve stays flat at roughly 2.0 (the maximum possible) for the first 25-30% of the population. This visually confirms that if we target the top 25%, we are getting nearly 100% positive results.\n",
    "- Decay: The Logistic model (Pink) sustains this high performance the longest. XGBoost (Green) is the first to degrade, dropping performance faster than the others after the top 20%.\n",
    "\n",
    "Gain Chart\n",
    "- Steep Ascent: All models rise sharply. By targeting just 40% of the population, we can capture roughly 75-80% of all positive cases.\n",
    "- Middle-Tier Performance: While Logistic wins at the very top (0-30%), RandomForest (Gold) performs slightly better in the \"middle\" of the list (40-80% depth), capturing marginally more positives than the others in that specific segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration (reliability) plots - one subplot per model\n",
    "n_models = len(models)\n",
    "plt.figure(figsize=(4 * n_models, 4))\n",
    "for i, (name, info) in enumerate(models.items(), start=1):\n",
    "    probs = np.asarray(info['y_proba'])\n",
    "    y_true = np.asarray(y_test)\n",
    "    # use quantile bins for even sample counts\n",
    "    frac_pos, mean_pred = calibration_curve(y_true, probs, n_bins=10, strategy='quantile')\n",
    "    plt.subplot(1, n_models, i)\n",
    "    plt.plot(mean_pred, frac_pos, \"s-\", label=name)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", alpha=0.6)\n",
    "    plt.fill_between(mean_pred, frac_pos, mean_pred, color='C0', alpha=0.05)\n",
    "    plt.xlabel('Mean predicted probability')\n",
    "    plt.ylabel('Fraction of positives')\n",
    "    plt.title(f'Calibration - {name}')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d516c6f",
   "metadata": {},
   "source": [
    "- Logistic Regression\n",
    "    - The \"Wobble\": Ideally, the red line should lie perfectly on the grey dashed diagonal. Here, it fluctuates.\n",
    "    - The Mid-Range Dip: Notice the point where the \"Mean predicted probability\" is 0.4 to 0.6. The actual \"Fraction of positives\" drops to around 0.2. In the middle of the pack, the model is over-estimating risk. It says \"there is a 50% chance,\" but the reality is closer to 20%.\n",
    "    - The Low-End Spike: At the very low end (predicted < 0.1), the actual positive rate spikes to > 0.3. This suggests the model is missing a segment of high-risk customers, incorrectly classifying them as very low risk.\n",
    "\n",
    "- RandomForest\n",
    "    - Best of the Batch: This curve follows the diagonal grey line more closely than the other two.\n",
    "    - The \"S\" Curve: It shows a slight \"sigmoid\" shape (over-predicting slightly in the middle, under-predicting at the tails), which is classic for tree-based models.\n",
    "    - Reliability: If this model predicts a probability of 0.8, the actual result is very close to 0.8. This makes the RandomForest probabilities the most \"interpretable\" as real-world risk percentages without further processing.\n",
    "\n",
    "- XGBoost\n",
    "    - Severe Misalignment: This chart is concerning for probability estimation.\n",
    "    - The Plateau: Look at the range between predicted probabilities of 0.4 and 0.9. The red line is almost flat, hovering around 0.4 actual positives. This means the model cannot distinguish between a \"40% risk\" and \"90% risk.\" It assigns higher probabilities, but the actual outcome doesn't change. The model is \"overconfident\" in this region.\n",
    "    - The Spike: It only becomes accurate again at the extreme high end (predicting > 0.95)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate classifiers and compare calibration (Platt sigmoid vs isotonic)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a time-safe / stratified calibration split from the original training set\n",
    "X_fit, X_calib, y_fit, y_calib = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.20, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "calibrated_models = {}\n",
    "brier_before = {}\n",
    "brier_after = {}\n",
    "\n",
    "# choose method depending on calibration sample size\n",
    "method = 'isotonic' if len(y_calib) >= 1000 else 'sigmoid'\n",
    "\n",
    "for name, info in models.items():\n",
    "    if name == 'Logistic':\n",
    "        base = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "    elif name == 'RandomForest':\n",
    "        base = RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "    elif name == 'XGBoost':\n",
    "        from xgboost import XGBClassifier\n",
    "        base = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "    else:\n",
    "        base = info['model']  # fallback\n",
    "\n",
    "    base.fit(X_fit, y_fit)\n",
    "\n",
    "    # Calibrate using held-out calibration set (cv='prefit' requires the estimator to be fitted)\n",
    "    calibrator = CalibratedClassifierCV(estimator=base, method=method, cv=5)\n",
    "    calibrator.fit(X_calib, y_calib)\n",
    "\n",
    "    # store calibrated model and calibrated probabilities on test set\n",
    "    y_proba_before = info['y_proba']\n",
    "    y_proba_after = calibrator.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    calibrated_models[name] = calibrator\n",
    "    brier_before[name] = brier_score_loss(y_test, y_proba_before)\n",
    "    brier_after[name] = brier_score_loss(y_test, y_proba_after)\n",
    "\n",
    "    # keep both versions in models dict for later use\n",
    "    models[name]['calibrated_model'] = calibrator\n",
    "    models[name]['y_proba_calibrated'] = y_proba_after\n",
    "\n",
    "# Print Brier scores\n",
    "print(\"Brier score (lower is better):\")\n",
    "for name in models.keys():\n",
    "    print(f\"{name}: before={brier_before[name]:.4f}  after={brier_after[name]:.4f}\")\n",
    "\n",
    "# Plot calibration curves before/after for each model\n",
    "from sklearn.calibration import calibration_curve\n",
    "plt.figure(figsize=(4*len(models), 4))\n",
    "for i, (name, info) in enumerate(models.items(), 1):\n",
    "    plt.subplot(1, len(models), i)\n",
    "    # before\n",
    "    frac_pos_b, mean_pred_b = calibration_curve(y_test, info['y_proba'], n_bins=10, strategy='quantile')\n",
    "    # after\n",
    "    frac_pos_a, mean_pred_a = calibration_curve(y_test, info['y_proba_calibrated'], n_bins=10, strategy='quantile')\n",
    "\n",
    "    plt.plot(mean_pred_b, frac_pos_b, \"s-\", label='before')\n",
    "    plt.plot(mean_pred_a, frac_pos_a, \"o-\", label=f'after ({method})')\n",
    "    plt.plot([0,1],[0,1],'k--', alpha=0.6)\n",
    "    plt.title(f'Calibration - {name}')\n",
    "    plt.xlabel('Mean predicted probability')\n",
    "    plt.ylabel('Fraction of positives')\n",
    "    plt.ylim(0,1)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a902a03",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "- XGBoost: Calibration was transformative for this model, reducing the Brier score from a poor 0.2948 to a leading 0.1005. The sigmoid method successfully corrected the extreme overconfidence and plateaus seen in the original \"before\" plot, making it the most reliable model for probability estimation.\n",
    "- RandomForest: This model was already the most naturally calibrated \"out of the box\" with a strong starting score of 0.1260. Applying the sigmoid calibration actually slightly increased the error to 0.1313, suggesting the uncalibrated version is likely superior for this specific dataset.\n",
    "- Logistic Regression: While Logistic Regression was the best for ranking/top-k precision, its Brier score worsened significantly from 0.1500 to 0.1792 after calibration. The sigmoid method appears to have struggled with the original model's \"wobble,\" indicating we should use the uncalibrated probabilities if we choose this model.\n",
    "\n",
    "### Model Selection\n",
    "- Although Logistic Regression initially had the best precision at the top 20%, the calibration process has made XGBoost the most robust \"all-around\" model. It now holds the lowest Brier Score (0.1005), meaning its predicted probabilities are the most honest and accurate of the three.\n",
    "- Correction of Overconfidence: The calibration fixed the severe \"plateau\" issue where the model previously couldn't distinguish between a 40% and 90% risk.\n",
    "- RandomForest & Logistic Note: Since calibration actually increased the Brier scores for Logistic (0.1500 to 0.1792) and RandomForest (0.1260 to 0.1313), you should not use the calibrated versions of those two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7184a",
   "metadata": {},
   "source": [
    "## 6. Churn via BG-NBD\n",
    "\n",
    "**BG-NBD Model:**\n",
    "- Estimates probability of being alive (P(alive))\n",
    "- Predicts expected future transactions\n",
    "\n",
    "**Decision: BG-NBD over Alternatives**\n",
    "- **Why BG-NBD?** Specifically designed for non-contractual settings.\n",
    "- **Comparison:** vs. simple survival models - accounts for transaction heterogeneity.\n",
    "- **Reasoning:** Proven in CLV literature for e-commerce.\n",
    "\n",
    "**Comparison with Classification Churn:**\n",
    "- Correlation between churn label and 1-P(alive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BG-NBD data preparation\n",
    "rfm_bgnbd = transactions.groupby('customer_id').agg(\n",
    "    frequency=('transaction_date', 'count'),\n",
    "    recency=('transaction_date', lambda x: (x.max() - x.min()).days),\n",
    "    monetary_value=('amount', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Subtract 1 to get repeat transactions\n",
    "rfm_bgnbd['frequency'] = rfm_bgnbd['frequency'] - 1\n",
    "\n",
    "# Merge with customers for T\n",
    "rfm_bgnbd = pd.merge(rfm_bgnbd, customers[['customer_id', 'signup_date']], on='customer_id', how='left')\n",
    "rfm_bgnbd = rfm_bgnbd.dropna(subset=['signup_date'])  # Remove customers without signup date\n",
    "rfm_bgnbd['T'] = (observation_date - rfm_bgnbd['signup_date']).dt.days\n",
    "\n",
    "rfm_bgnbd.loc[rfm_bgnbd['frequency'] == 0, 'recency'] = 0\n",
    "\n",
    "# Fit BG-NBD\n",
    "bgf = BetaGeoFitter(penalizer_coef=0.01)\n",
    "bgf.fit(rfm_bgnbd['frequency'], rfm_bgnbd['recency'], rfm_bgnbd['T'])\n",
    "\n",
    "# P(alive)\n",
    "p_alive = bgf.conditional_probability_alive(rfm_bgnbd['frequency'], rfm_bgnbd['recency'], rfm_bgnbd['T'])\n",
    "rfm_bgnbd['p_alive'] = p_alive\n",
    "\n",
    "# Compare with churn\n",
    "churn_compare = pd.merge(churn_df[['customer_id', 'churn']], rfm_bgnbd[['customer_id', 'p_alive']], on='customer_id')\n",
    "print(\"Churn vs P(alive) correlation:\")\n",
    "churn_compare[['churn', 'p_alive']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction horizon (30 days)\n",
    "t_next = 30\n",
    "\n",
    "# Calculate expected purchases for each customer\n",
    "rfm_bgnbd['expected_purchases_30_days'] = bgf.conditional_expected_number_of_purchases_up_to_time(\n",
    "    t_next, \n",
    "    rfm_bgnbd['frequency'], \n",
    "    rfm_bgnbd['recency'], \n",
    "    rfm_bgnbd['T']\n",
    ")\n",
    "\n",
    "print(\"Top 5 customers by expected purchases in next 30 days:\")\n",
    "rfm_bgnbd[['customer_id', 'expected_purchases_30_days']].sort_values(by='expected_purchases_30_days', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee245139",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- The BG-NBD model shows strong validity with a -0.71 correlation between churn labels and P(alive) probabilities. This negative correlation is expected and confirms the model's ability to distinguish active from inactive customers. The probabilistic approach provides more nuanced customer alive estimates compared to binary churn classification.\n",
    "\n",
    "- The strong correlation validates BG-NBD as a superior alternative to simple recency-based churn detection. Businesses can use P(alive) for personalized engagement strategies, such as increasing touch frequency for customers with declining probabilities. This approach enables more efficient resource allocation by focusing retention efforts on customers most likely to respond positively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc06a21",
   "metadata": {},
   "source": [
    "## 7. Churn with Survival Analysis\n",
    "\n",
    "**Survival Analysis:**\n",
    "- Models time-to-churn\n",
    "- Cox PH: Proportional hazards with covariates\n",
    "- Weibull: Parametric baseline\n",
    "\n",
    "**Decision: Cox PH over Weibull**\n",
    "- **Cox for flexibility:** Handles covariates without assuming distribution.\n",
    "- **Comparison:** Weibull simpler but less flexible for complex relationships.\n",
    "- **Reasoning:** Covariates (frequency, monetary) important for churn prediction.\n",
    "\n",
    "**Outputs:**\n",
    "- Survival curves\n",
    "- Expected remaining lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival data\n",
    "survival_df = []\n",
    "for cid in transactions['customer_id'].unique():\n",
    "    cust_trans = transactions[transactions['customer_id'] == cid].sort_values('transaction_date')\n",
    "    first_date = cust_trans['transaction_date'].min()\n",
    "    last_date = cust_trans['transaction_date'].max()\n",
    "    # Duration is the full window of observation for that customer\n",
    "    duration = (observation_date - first_date).days\n",
    "    # Event is 1 if they haven't shopped in 60 days (churned)\n",
    "    # Event is 0 if they shopped recently (censored/still alive)\n",
    "    event = 1 if (observation_date - last_date).days > 60 else 0\n",
    "    \n",
    "    survival_df.append({\n",
    "        'customer_id': cid, \n",
    "        'duration': duration, \n",
    "        'event': event, \n",
    "        'frequency': len(cust_trans), \n",
    "        'monetary': cust_trans['amount'].mean() # Use mean to avoid duration correlation\n",
    "    })\n",
    "\n",
    "survival_df = pd.DataFrame(survival_df)\n",
    "\n",
    "# Fit Cox PH\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(survival_df[['duration', 'event', 'frequency', 'monetary']], duration_col='duration', event_col='event')\n",
    "\n",
    "print(\"Cox PH Summary:\")\n",
    "cph.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8a921",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- Frequency: The Strongest Predictor\n",
    "    - Coefficient (coef): -0.043868 — A negative value means that as frequency increases, the hazard (risk of churn) decreases.\n",
    "    - Hazard Ratio (exp(coef)): 0.957080 — This is the most important number. It means that for every additional purchase a customer makes, their risk of churning drops by approximately 4.3% ($1 - 0.957$).\n",
    "    - Significance (p): 2.99e-55 — This value is extremely close to zero, meaning frequency is a statistically \"highly significant\" predictor of how long a customer will stay with you.\n",
    "    - Z-score: -15.65 — Such a large negative Z-score confirms that frequency has a very strong protective effect against churn.\n",
    "- Monetary: Not Statistically Significant\n",
    "    - Hazard Ratio (exp(coef)): 0.999642 — While this is slightly below 1 (suggesting a 0.03% risk reduction per dollar spent), it is practically negligible.\n",
    "    - Significance (p): 0.5697 — In most scientific and business contexts, a p-value above 0.05 means the result is not statistically significant.\n",
    "    - Confidence Interval: Notice that the exp(coef) upper 95% is 1.000877. Since this range includes 1.0, we cannot say with certainty whether spending more money increases, decreases, or has no effect on churn risk.\n",
    "\n",
    "### Actionable Step:\n",
    "Loyalty via Habits\n",
    "- Frequency is your \"North Star\" for retention.\n",
    "- Focus marketing on repeat purchase behaviors rather than single high-value transactions.\n",
    "\n",
    "Spend vs. Retention\n",
    "- High-spending customers are not necessarily \"safer\" from churn than low-spending ones.\n",
    "- Don't assume \"VIP\" spenders will stay naturally; they need frequency-driving engagement too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Plotting the Coefficients (Hazard Ratios) - This is what cph.plot() does\n",
    "plt.figure(figsize=(8, 4))\n",
    "cph.plot()\n",
    "plt.title('Impact of Covariates on Churn Risk (Hazard Ratios)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 2. Plotting the actual Baseline Survival Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "cph.baseline_survival_.plot() # Use this for the survival probability over time\n",
    "plt.title('Baseline Survival Curve (Average Customer)')\n",
    "plt.xlabel('Days since first purchase')\n",
    "plt.ylabel('Probability of remaining active')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 3. Predict Expectation (Mean Lifetime)\n",
    "# Ensure columns match the training data exactly\n",
    "avg_data = pd.DataFrame({\n",
    "    'frequency': [survival_df['frequency'].mean()],\n",
    "    'monetary': [survival_df['monetary'].mean()]\n",
    "})\n",
    "\n",
    "# This returns the total expected life from day 0\n",
    "expected_life = cph.predict_expectation(avg_data).iloc[0]\n",
    "print(f\"Total expected customer lifetime: {expected_life:.1f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09522ab7",
   "metadata": {},
   "source": [
    "- Impact of Covariates on Churn Risk (Hazard Ratios)\n",
    "    - The forest plot visualizes how strongly different customer behaviors impact the \"hazard\" (the risk) of churning.\n",
    "    - Frequency is the Key Driver: The point estimate for frequency is significantly to the left of the zero line (at approximately -0.044). This confirms that higher purchase frequency has a strong \"protective\" effect, lowering the risk of churn.\n",
    "    - Monetary Impact is Negligible: The dot for monetary sits almost directly on the zero line. This indicates that the total amount a customer spends has very little statistical impact on how long they stay with the brand compared to how often they shop.\n",
    "    - Confidence Intervals: The narrow bars around the points show high precision in these estimates, especially for frequency, where the entire interval is well below zero.\n",
    "\n",
    "- Baseline Survival Curve (Average Customer)\n",
    "    - This curve shows the probability that an \"average\" customer remains active (has not churned) over time since their first purchase.\n",
    "    - Initial Stability (0–75 Days): The curve remains flat and near 1.0 (100%) for the first two months. During this period, very few customers meet the 60-day inactivity threshold to be considered \"churned\".\n",
    "    - The Inflection Point (~100 Days): After roughly 100 days, the survival probability begins to drop steadily. This is the critical window where standard customer behavior starts to diverge into either long-term loyalty or churn.\n",
    "    - The Half-Life (~275 Days): At approximately 275 days since the first purchase, the probability of remaining active drops to 0.5 (50%). This means half of your typical customers will have churned by this point in their journey.\n",
    "    - Full Decay: By 350+ days, the survival probability approaches zero, indicating that very few customers in this dataset stay active for over a full year without a significant break."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd61fcd",
   "metadata": {},
   "source": [
    "## 8. CLV Modeling\n",
    "\n",
    "**Two Approaches:**\n",
    "1. BG-NBD + Gamma-Gamma\n",
    "2. Survival + Gamma-Gamma\n",
    "\n",
    "**Gamma-Gamma Model:**\n",
    "- Estimates expected monetary value per transaction\n",
    "\n",
    "**Decision: Both Approaches**\n",
    "- **BG-NBD + Gamma-Gamma:** Standard for transactional CLV.\n",
    "- **Survival + Gamma-Gamma:** Incorporates time-to-churn.\n",
    "- **Comparison:** BG-NBD assumes alive customers; Survival handles censored data.\n",
    "- **Reasoning:** Compare robustness for different scenarios.\n",
    "\n",
    "**CLV Formula:**\n",
    "CLV = Expected Transactions × Expected Monetary Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53143d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma-Gamma for monetary\n",
    "\n",
    "returning = rfm_bgnbd[rfm_bgnbd['frequency'] > 0]\n",
    "print(f\"Monetary value mean: ${rfm_bgnbd['monetary_value'].mean():.2f}\")\n",
    "print(f\"Transactions amount mean: ${transactions['amount'].mean():.2f}\")\n",
    "\n",
    "try:\n",
    "    ggf = GammaGammaFitter(penalizer_coef=1.0)  # Further increased penalizer\n",
    "    ggf.fit(returning['frequency'], returning['monetary_value'])\n",
    "    exp_monetary = ggf.conditional_expected_average_profit(rfm_bgnbd['frequency'], rfm_bgnbd['monetary_value'])\n",
    "except:\n",
    "    print(\"Gamma-Gamma failed to converge, using simple average monetary value\")\n",
    "    exp_monetary = rfm_bgnbd['monetary_value']  # Fallback to simple average\n",
    "\n",
    "print(f\"Expected monetary mean: ${exp_monetary.mean():.2f}\")\n",
    "\n",
    "rfm_bgnbd['clv_bgnbd'] = ggf.customer_lifetime_value(\n",
    "    bgf,\n",
    "    rfm_bgnbd['frequency'],\n",
    "    rfm_bgnbd['recency'],\n",
    "    rfm_bgnbd['T'],\n",
    "    rfm_bgnbd['monetary_value'],\n",
    "    time=12, # 12 months (roughly 365 days)\n",
    "    discount_rate=0.01 # Monthly discount rate\n",
    ")\n",
    "individual_expected_life = cph.predict_expectation(survival_df[['frequency', 'monetary']])\n",
    "# (Individual Expected Life / 30) converts days to months for monetary projection\n",
    "survival_df['clv_survival'] = (individual_expected_life / 30) * exp_monetary\n",
    "\n",
    "print(\"CLV Statistics:\")\n",
    "print(f\"BG-NBD CLV mean: ${rfm_bgnbd['clv_bgnbd'].mean():.2f}\")\n",
    "print(f\"Survival CLV mean: ${survival_df['clv_survival'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8225fca3",
   "metadata": {},
   "source": [
    "- Current Average Transaction: $58.26.\n",
    "- Expected Monetary Mean: $89.26. The Gamma-Gamma model suggests that while the population average is lower, the \"active\" or \"returning\" segments actually have a significantly higher expected spend per transaction.\n",
    "- The BG-NBD has the mean CLV of $1,412.75. It assumes the customer will continue to \"buy till they die\" in a non-contractual cycle, and is often more optimistic, projecting value over a longer potential horizon, while Survival model, with the mean CLV of $757.21 estimates value based on the specific time remaining until a predicted churn event, and are grounded in the steep decline of the baseline curve observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88d46f",
   "metadata": {},
   "source": [
    "## 9. Final Business Question\n",
    "\n",
    "**Question:** If budget allows retention of only 20% of customers, which 20% to prioritize?\n",
    "\n",
    "**Strategies:**\n",
    "1. High churn probability (from classification)\n",
    "2. Low P(alive) (from BG-NBD)\n",
    "3. High CLV × High churn risk (combined approach)\n",
    "\n",
    "**Decision: Three Strategies**\n",
    "- **Strategy 1:** Direct churn focus.\n",
    "- **Strategy 2:** Behavioral probability.\n",
    "- **Strategy 3:** Value-weighted risk.\n",
    "- **Comparison:** Strategy 3 balances value and risk for optimal ROI.\n",
    "- **Reasoning:** Retention should prioritize high-value at-risk customers.\n",
    "\n",
    "**Implementation:** Rank and select top 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7241f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a base dataframe from the Test Set (ensures size 3000 and correct alignment)\n",
    "# X_test index is 'customer_id' as defined in Cell 24\n",
    "comparison_df = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "# 2. Add Strategy 1: Calibrated XGBoost Probabilities\n",
    "# These were calculated on X_test_scaled in Cell 36\n",
    "comparison_df['xgb_churn_prob'] = models['XGBoost']['y_proba_calibrated']\n",
    "\n",
    "# 3. Reset index to merge other data on 'customer_id'\n",
    "comparison_df = comparison_df.reset_index()\n",
    "\n",
    "# 4. Merge other model outputs\n",
    "# Add BG-NBD results (Strategy 2)\n",
    "comparison_df = pd.merge(comparison_df, rfm_bgnbd[['customer_id', 'p_alive', 'clv_bgnbd']], on='customer_id', how='left')\n",
    "\n",
    "# Add Survival results (Strategy 3)\n",
    "comparison_df = pd.merge(comparison_df, survival_df[['customer_id', 'clv_survival']], on='customer_id', how='left')\n",
    "\n",
    "# Calculate Partial Hazard for Strategy 3 using the CPH model from Cell 43\n",
    "# We calculate it for the whole survival_df then merge to ensure alignment\n",
    "survival_risk = survival_df[['customer_id', 'frequency', 'monetary']].copy()\n",
    "survival_risk['partial_hazard'] = cph.predict_partial_hazard(survival_risk[['frequency', 'monetary']])\n",
    "comparison_df = pd.merge(comparison_df, survival_risk[['customer_id', 'partial_hazard']], on='customer_id', how='left')\n",
    "\n",
    "# Fill NaNs with 0 (for any test customers not present in BG-NBD/Survival subsets)\n",
    "comparison_df.fillna(0, inplace=True)\n",
    "\n",
    "# --- Execute Strategy Selection ---\n",
    "top_k = int(0.2 * len(comparison_df))\n",
    "\n",
    "# Strategy 1: High Churn Prob (XGBoost Focus)\n",
    "strategy1 = comparison_df.sort_values('xgb_churn_prob', ascending=False).head(top_k)\n",
    "\n",
    "# Strategy 2: Low P(alive) (BG-NBD Focus)\n",
    "strategy2 = comparison_df.sort_values('p_alive', ascending=True).head(top_k)\n",
    "\n",
    "# Strategy 3: High Value-at-Risk (Survival & CLV Focus)\n",
    "comparison_df['value_at_risk'] = comparison_df['clv_survival'] * comparison_df['partial_hazard']\n",
    "strategy3 = comparison_df.sort_values('value_at_risk', ascending=False).head(top_k)\n",
    "\n",
    "# --- Comparison Results ---\n",
    "print(f\"Total customers evaluated: {len(comparison_df)}\")\n",
    "print(f\"Strategy 1 (XGBoost) Avg CLV at Risk: ${strategy1['clv_bgnbd'].mean():.2f}\")\n",
    "print(f\"Strategy 2 (BG-NBD) Avg CLV at Risk: ${strategy2['clv_bgnbd'].mean():.2f}\")\n",
    "print(f\"Strategy 3 (Survival/Value) Avg CLV at Risk: ${strategy3['clv_bgnbd'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234b315",
   "metadata": {},
   "source": [
    "- By using Strategy 3 (Survival-based Value-at-Risk), we can prioritize the 20% of customers who represent the highest financial loss. This approach allows us to protect $1,223 per customer targeted, compared to only $18 per customer if we relied on classification alone. We should implement Strategy 3 immediately to maximize the ROI of our retention spend.\n",
    "\n",
    "- The reason Strategy 2 (BG-NBD) is so low ($0.08) is that p_alive is a very strict probabilistic measure. Many customers in the test set likely had very low p_alive scores but also very low frequency, resulting in a CLV that was nearly zero. This confirms that Survival Analysis (Cox PH) is a much more robust way to calculate \"risk\" for your specific dataset than the standard BG-NBD approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bbf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Total Value at Risk (Sum of CLV for the top 20%)\n",
    "total_risk_s1 = strategy1['clv_bgnbd'].sum()\n",
    "total_risk_s2 = strategy2['clv_bgnbd'].sum()\n",
    "total_risk_s3 = strategy3['clv_bgnbd'].sum()\n",
    "\n",
    "print(f\"--- TOTAL REVENUE AT RISK BY STRATEGY ---\")\n",
    "print(f\"Strategy 1 (XGBoost):    ${total_risk_s1:,.2f}\")\n",
    "print(f\"Strategy 2 (BG-NBD):     ${total_risk_s2:,.2f}\")\n",
    "print(f\"Strategy 3 (Survival):  ${total_risk_s3:,.2f}\")\n",
    "\n",
    "# Calculate 'Lift' over the standard ML approach (Strategy 1)\n",
    "lift = total_risk_s3 / total_risk_s1\n",
    "print(f\"\\nBusiness Impact: Strategy 3 protects {lift:.1f}x more revenue than Strategy 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "strategies = ['XGBoost (Churn Prob)', 'BG-NBD (P-Alive)', 'Survival (Value-at-Risk)']\n",
    "totals = [total_risk_s1, total_risk_s2, total_risk_s3]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(strategies, totals, color=['#3498db', '#95a5a6', '#e74c3c'])\n",
    "\n",
    "# Add labels on top of bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 10000, f'${yval:,.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title('Total Revenue Protected by Each Retention Strategy (Top 20% Budget)', fontsize=14)\n",
    "plt.ylabel('Total CLV at Risk ($)')\n",
    "plt.ylim(0, max(totals) * 1.2) # Add space for labels\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "print(\"Saving models...\")\n",
    "\n",
    "# 1. Save Calibrated XGBoost and Scaler\n",
    "joblib.dump(models['XGBoost']['calibrated_model'], 'models/xgb_calibrated.joblib')\n",
    "joblib.dump(scaler, 'models/scaler.joblib')\n",
    "\n",
    "# 2. Save Lifetimes models (BG-NBD and Gamma-Gamma)\n",
    "bgf.save_model('models/bgf_model.pkl')\n",
    "ggf.save_model('models/ggf_model.pkl')\n",
    "\n",
    "# 3. Save Survival Model (CoxPH)\n",
    "joblib.dump(cph, 'models/cph_model.joblib')\n",
    "\n",
    "print(\"All models saved successfully to 'models/' folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
